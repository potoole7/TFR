\documentclass{article}

\usepackage[top=3cm, bottom=3cm, left=3cm,right=3cm]{geometry}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{titlesec}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage{todonotes}
\usepackage{pdflscape}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{pdfpages}
\usepackage{setspace} 
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage{tikz}
\usepackage[colorlinks=true,citecolor=blue, linkcolor=blue]{hyperref}
\usepackage{multirow}
\usepackage{todonotes}
\setlength{\tabcolsep}{5pt}
%%\setlength{\parindent}{0pt}
\usepackage[parfill]{parskip}
\renewcommand{\arraystretch}{1.5}

% \renewcommand\Affilfont{\itshape\footnotesize}
% \def\ci{\perp\!\!\!\perp}

% \renewcommand\Affilfont{\itshape\footnotesize}
% \linespread{1.5}

% Define a custom note command for general notes
\newcommand{\mynote}[1]{\todo[color=yellow!40,inline]{#1}}

% Nature Bibliography style
\usepackage[backend=biber,style=nature]{biblatex}
\addbibresource{library.bib} 


\title{Extreme Value Theory Notes}
\author{Paddy O'Toole}
\date{\today}

\begin{document}

\maketitle

\tableofcontents


\todo{Limit number of warnings from chktex}
\todo{Replace notes in text with calls to mynote}
% \todo{Add macros for Y-i, a_mid_i, b_mid_i?}
% \todo{Get snippets working with vimtex(just use c-s)}
\todo{Add introduction, including stuff from Coles book?}
\todo{number equations}
\todo{Remove numbers for un-referenced equations}
\todo{Tidy bibtex references}
\todo{Include authors names rather than numbering for references}

\section{Introduction}\label{sec:intro}

\section{Univariate extremes}\label{sec:uni}

\section{Conditional extremes model}\label{sec:ce}

\todo{Look back into page 500 and equation 1.5, needed here?}
\todo{Uses beta for GPD scale, but beta used for normalising function, and want sigma for moments of Z, need to have consistent notation}
\todo{Improve equation referencing etc}

Description of model comes from \cite{Heffernan2004}, with the addition of using Laplace margins and constraining the normalising function parameters coming from \cite{Keef2013}.

\subsection{Data}

\begin{itemize} 
  \item Continuous vector variable $\bm{X} = (X_1, X_2, \ldots, X_d)$. 
  \item Want to estimate $\mathbb{P}(\bm{X} \in \bm{C})$, where $\bm{C}$ is an extreme set such that $\forall \bm{X} \in \bm{C}$, at least one component of $\bm{X}$ i extreme. 
  \item $C_i$ corresponds to the part of $C$ for which $X_i$ is the largest component of $\bm{X}$, by quantiles of the marginal distribution. 
  \item $C_i = C \cap \left\{ \bm{x} \in \mathbb{R}^d: F_{X_i}(x_i) > F_{X_j}(x_j); j = 1, \ldots, d; j \ne i \right\}$ for $i = 1, \ldots, d$., where $F_{X_i}$ is the marginal distribution function of $X_i$. % \todo{Double check capitalisation on Fs}
  \item Ignore subsets $C \cap \left\{ \bm{X} \in \mathbb{R}^d: F_{X_i}(X_i) = F_{X_j}(x_j) \text{for some} j \ne i \right\}$, as they are null sets. 
  \item C is an extreme set if all $x_i$-values in non-empty $C_i$ fall in upper tail of $F_{X_i}$, i.e.\ if $\nu_{X_i} = \inf_{\bm{x} \in C_i}{(x_i)}$, then $F_{X_i}(\nu_{X_i})$ is close 1 for $i = 1, \ldots, d$, so 
  \[
  \mathbb{P}(\bm{X} \in \bm{C}) = \sum_{i=1}^{d}{\mathbb{P}(\bm{X} \in C_i)} = \sum_{i=1}^{d}\textcolor{blue}{\mathbb{P}(\bm{X} \in C_i\mid X_i > \nu_{X_i})}\textcolor{red}{\mathbb{P}(X_i> \nu_{X_i})}
  \]
  \item \textcolor{red}{red probability is estimated with marginal extreme value model}, while the \textcolor{blue}{blue probability is estimated using an extreme value model for the dependence structure}. 
\end{itemize}

\subsection{Marginal extremes model}

\begin{itemize}
  \item Model marginal tail of $X_i$ with Generalised Pareto Distribution (GPD):
    % \[
    %   \mathbb{P}(X_i > x + u_{X_i} \mid X_i > u_{X_i}) = {(1 + \xi_ix/\sigma_i)}_{+}^{-1/\xi_i}, x > 0 \todo{Fix this!}
    % \]
    where $u_{X_i}$ is a high threshold, $\xi_i$ is the shape parameter, $\sigma_i$ is the scale parameter, and ${x}_{+} = \max(x, 0)$.
  \item Require a model for complete marginal distribution $F_{X_i}$  of $X_i$, so need to describe all $X_j$ values that can occur with any large $X_i$ value, which leads to the following piecewise semiparameteric model:
    \[
      \hat{F}_{X_i}(x) = \begin{cases}
        1 - \{ 1 - \tilde{F}_{X_i}(u_{X_i})\} \left\{1 + \xi_i(x - u_{X_i})/\sigma_i\right\}_{+}^{-1/\xi_i} & \text{if } x > u_{X_i} \\
        \tilde{F}_{X_i}(x) & \text{if } x \le u_{X_i}
      \end{cases}
    \]
    where $\tilde{F}_{X_i}$ is the empirical distribution function of the $X_i$ values. 
  \item This gives us estimates of $\mathbb{P}(X_i < \nu_{X_i})$.
\end{itemize}

\subsection{Marginal transformation}

\todo{Describe need for marginal transformation to estimate dependence model}

\subsubsection{Gumbel transformation}

\begin{align*}
  Y_i &= -\log[-\log\{\hat{F}_{X_i}(X_i)\}], i = 1, \ldots, d \\
      &= t_i(X_i; \phi_i, \tilde{F}_{X_i}(X_i)) \\
      &= t_i(X_i),
\end{align*}
where $\phi_i = (\sigma_i, \xi_i)$ are marginal parameters. \\
This gives $\mathbb{P}(Y_i \le y) = \exp(-\exp(-y)) \implies \mathbb{P}(Y_i > y) \sim \exp(-y) \text{ as } y \rightarrow \infty$, so $Y_i$ has an exponential upper tail. 

\subsubsection{Laplace transformation}
The Laplace transformation detailed in \cite{Keef2013} is given by
\[
  Y_i = \begin{cases}
    \log\{2F_{X_i}(x_i)\} &\text{ for } X_i < F_{X_i}^{-1}(0.5) \\
    -\log\{2[1 - F_{X_i}(x_i)]\} &\text{ for } X_i \ge F_{X_i}^{-1}(0.5) \\
  \end{cases}
\]
which means that 
\[
  \mathbb{P}(Y_i \le y) = \begin{cases}
    \exp(y)/2 &\text{ for } y < 0 \\
    1-\exp(-y)/2 &\text{ for } y \ge 0 \\
  \end{cases}
\]
so that both tails of $Y_i$ are exponential, and so for any $u > 0$, the distribution of $Y_i - u \mid Y_i > u$ and $(-Y_i + u) \mid Y_i \le -u$ are exponential with mean 1. 
This greatly simplifies the normalising functions seen in section \ref{subsubsec:norm}, as for Gumbel margins a more complex normalising function is required for negatively associated variables. 

\subsection{Asymptotic dependence}

\begin{itemize}
  \item $
      \lim_{y \rightarrow \infty}\{\mathbb{P}(\bm{Y}_{-i} \mid Y_i > y)\} = \begin{cases}
      0 &\text{for asymptotic independence} \\
      \ne 0 &\text{for asymptotic dependence}, 
    \end{cases}
    $ \\
  where $\bm{Y}_{-i} = (Y_1, Y_2, \ldots, Y_{i-1}, Y_{i+1}, \ldots, Y_d)$. 
  \item Existing methods for multivariate extremes (e.g.\ max-stable processes, copulas) can only model $\mathbb{P}(\bm{X} \in C)$ under asymptotic dependence.\todo{revisit this and talk a bit more about it!}
\end{itemize}

\subsubsection{Limit assumption}

\begin{itemize}
  \item For each $Y_i$, want to estimate $\mathbb{P}(Y_{-i} \le y_{-i} \mid Y_i = y_i)$ as $y \rightarrow \infty$. 
  \item We require the limiting distribution to be non-degenerate for all margins (see section\ref{sec:uni})
  \item Therefore, assume for every $i$ that there are vector normalising functions $\bm{a}_{\mid i}(y_i),\bm{b}_{\mid i}(y_i), \in \mathbb{R} \rightarrow \mathbb{R}^{(d-1)}$ such that for fixed $\bm{z}_{\mid i}$, 
    \[
      \lim_{y_i \rightarrow \infty}\{\mathbb{P}(\bm{Y}_{-i} \le \bm{a}_{\mid i}(y_i) + \bm{b}_{\mid i}(y_i)\bm{z}_{\mid i} \mid Y_i = y_i)\} = \bm{G}_{\mid i}(\bm{z}_{\mid i})
    \] \todo{revisit, probably wrong}
    where all margins of $\bm{G}_{\mid i}$ are non-degenerate, so 
    \[
    \lim_{z \rightarrow \infty}{\bm{G}_{j \mid i}(\bm{z})} = 1, \forall j \ne i
    \] (no mass at $+\infty$, some allowed at $-\infty$).
  \item Alternatively, the standardised variables
    \[
      \bm{Z}_{\mid i} = \frac{\bm{Y}_{-i} - \bm{a}_{\mid i}(y_i)} {\bm{b}_{\mid i}(y_i)}
    \]
    have the property that 
    \[
      \lim_{y_i \rightarrow \infty}\{\mathbb{P}(\bm{Z}_{\mid i} \le \bm{z}_{\mid i} \mid Y_i = y_i)\} = \bm{G}_{\mid i}(\bm{z}_{\mid i})
    \]
  \item Conditional on $Y_i > u_i$ as $u_i \rightarrow \infty$, $Y_i  - u_i$ and $\bm{Z}_{\mid i}$ are independent in the limit with limiting marginal distributions being exponential and $G_{\mid i}$ respectively:
    \[
      \mathbb{P}(\bm{Z}_{\mid i} \le \bm{z}_{\mid i}, Y_i - u_i = y_i \mid Y_i > u_i) \rightarrow G_{\mid i}(\bm{z}_i) \exp(-y) \text{ ,as } u_i \rightarrow \infty
    \]
  \item For each $j \ne i$, 
    \[
      Z_{j\mid i} = \frac{Y_j - a_{j\mid i}(y_i)}{b_{j\mid i}(y_i)} \sim G_{j\mid i}(z_{j\mid i}) \text { given } Y_i = y \text{ as } y_i \rightarrow \infty
    \]
    $\implies G_{j \mid i}$ is the marginal distribution of $G_{\mid i}$ associated with $Y_j$.
\end{itemize}

\subsubsection{Normalisation} \label{subsubsec:norm}

\begin{itemize}
  \item Under Gumbel margins, normalising functions have simple form for class of positively associated variable, but more complex for negatively associated variables:
    \todo{Fix, missing curly bracket}
    % \begin{align*}
    %   \bm{a}_{\mid i}(y) &= \bm{\alpha}_i y + I_{\{\bm{\alpha}_{\mid i} = 0, \bm{\beta}_{\mid i} < 0\}}\{_\bm{\gamma}_{\mid i} - \bm{\delta}_{\mid i}\log(y)\}, \\
    %   \bm{b}_{\mid i}(y) &= y^{\bm{\beta}_{\mid i}}, 
    % \end{align*}
    where $\bm{\alpha}_{\mid i}, \bm{\beta}_{\mid i}, \bm{\gamma}_{\mid i}, \bm{\delta}_{\mid i}$ are vector constants, $\bm{\alpha}_{\mid i} \in [0, 1], \bm{\beta}_{\mid i} \in (-\infty, 1], \bm{\gamma}_{\mid i} \in (-\infty, \infty) , \bm{\delta}_{\mid i} \in [0, 1]$. 
  \item Under Laplace marginals, as in \cite{Keef2013}, the indicator term disappears, and we have that $\bm{\alpha}_{\mid i} \in [-1, 1]$. 
  \item $\alpha_{j \mid i}$ controls the level of association between $Y_j$ and large $Y_i$, with positive and negative values indicating positive and negative asymptotic dependence, respectively, and values closer to 0 indicating stronger asymptotic independence. 
  % \item $\beta_{j \mid i}$ controls the rate of convergence to the limiting
    % distribution, with values closer to 1 indicating slower convergences %
    % came from copilot, is this right??
  \item $\beta_{j \mid i}$ controls the spread $\ldots$ \todo{Fill in more formally}
\end{itemize}

\todo{Add interpretation of alpha and beta from Keef paper (page 400)}

\subsection{Conditional dependence model}

\begin{itemize}
  \item $\bm{G}_{\mid i}$ is modelled nonparameterically as the empirical distribution of 
    \[ 
      \bm{Z}_{\mid i} = \frac{\bm{Y}_{-i} - \bm{\alpha}_{\mid i}(Y_i)}{(Y_i)^{\bm{\beta}_{\mid i}}}
    \]
  \item All $d$ different conditional distributions are estimated separately.
\end{itemize}

\todo{on page 508, do I need to talk about Z hat? How G hat is its empirical distribution?}

\subsection{Extrapolation}

\begin{itemize}
  \item Can simulate from $\bm{X}|\bm{X}_i > \nu_{X_i}$ by simulating from $\bm{Y}|\bm{Y}_i > y_i$ and transforming back to $\bm{X}$ space, using the following algorithm:
    \begin{enumerate}
      \item Simulate $Y_i$ from the transformed marginal distribution conditional on exceeding $t_i(\nu_{X_i})$.
      \item Sample $\bm{Z}_{\mid i}$ from $\hat{G}_{\mid i}$, independently of $Y_i$. 
      \item Obtain $\bm{Y}_{-i} = \bm{\alpha}_{\mid i}(Y_i) + (Y_i)^{\bm{\beta}_{\mid i}}\bm{Z}_{\mid i}$.
      \item Transform $\bm{Y} = (\bm{Y}_{-i}, Y_i)$ back to the original scale by using the inverse of the marginal transformation. 
      \item the resulting vector $\bm{X}$ is a simulated value from $\bm{X} \mid X_i > \nu_{X_i}$.
    \end{enumerate}
  \item This algorithm can be used to estimate $\mathbb{P}(\bm{X} \in C_i \mid X_i > \nu_{X_i})$ by evaluating it as the long run proportion of the generated sample that falls in $C_i$. 
\end{itemize}

\subsection{Diagnostics}

\begin{itemize}
  \item Marginals are diagnosed as in univariate EVT (threshold with mean excess plot etc.\ , model fit assessed with probability and quantile plots)
  \item Dependence:
    \begin{itemize}
      \item Normalised variable $\bm{Z}_{\mid i}$ must have stable distribution over range of $Y_i$ used for estimation and evaluation. 
      \item Independence of $\bm{Z}_{\mid i}$ and $Y_i$ given $Y_i > u_i$ for high threshold $u_i$ is explored (similar to diagnostics for `ordinary' linear model)
      \item Can also use standard statistical tests for independence. 
    \end{itemize}
\end{itemize}

\subsection{Inference}

\begin{itemize}
  \item Inference for marginal parameters $\bm{\psi}$ and dependence parameters $\theta$ is done stepwise (loss of efficiency deemed small, estimation methods much easier)
\end{itemize}

\subsubsection{Marginal Estimation}

\begin{itemize}
  \item $d$ univariate distributions estimated jointly assuming independence between components in LL:
    \[
      \log\{L(\bm{\psi})\} = \sum_{i=1}^{d}{\sum_{j=1}^{n_{u_{X_i}}}{\log\{\hat{f}_{X_i}(x_{i \mid i, k})\}}}
    \]
    $f_{X_i}$ is the density associated with semiparameteric marginal $\hat{F}_{X_i}$, and $n_{u_{X_i}}$ is the number of exceedances of $u_{X_i}$.
  \item Equivalent to fitting GPD for each margin to excesses over marginal threshold
  \item Can estimate GPD using more complicated methods, such as extreme value GAMs and spatiotemporal models which allow for covariates and information borrowing between marginals.
\end{itemize}

\subsubsection{Single conditional}

\begin{itemize}
  \item Want to estimate $\bm{\theta}_{\mid i}$ under minimal assumptions about $\bm{G}_{\mid i}$. \todo{Do I explicitely define theta and psi as parameters of G and marginals anywhere?}
  \item Assume $Z_{\mid i}$ has two finite marginal moments, $\bm{\mu}_{\mid i}$ and $\bm{\sigma}_{\mid i}$ (Note: $\bm{\sigma}_{\mid i}$ is the vector of standard deviations of $\bm{Z}_{\mid i}$, separate to parameter of GPD) \todo{Will have to change one of these, see Christian's paper (he uses psi and nu)}
  \item Therefore $\bm{Y}_{-i} \mid Y_i = y$ for $y > u_{Y_i}$ has vector mean and standard deviation
    \begin{align*}
      \bm{\mu}_{\mid i}(y) &= \bm{a}_{\mid i}(y) + \bm{\mu}_{\mid i}{\bm{b}_{\mid i}}(y), \\
      \bm{\sigma}_{\mid i}(y) &= \bm{\sigma}_{\mid i}{\bm{b}_{\mid i}}(y),
    \end{align*}
  \item (Note: $\bm{\mu}_{\mid i}(y)$ is the vector mean, while $\bm{\mu}_{\mid i}$ are it's respective parameters)
  \item $\implies$ $(\bm{\theta}_{\mid i}, \bm{\lambda}_{\mid i} = (\bm{\mu}_{\mid i}, \bm{\sigma}_{\mid i}))$ are parameters of multivariate regression model to be estimated. 
  \item Assume components of $\bm{Z}_{\mid i}$ are independent and Gaussian, for simplicity and convenience. 
  \item Independence assumption is reasonable as $\bm{\theta}_{\mid i}$ determines only the marginal behaviour of the conditional distribution. 
  \item The objective function for the point estimation of $(\bm{\theta}_{\mid i}, \bm{\lambda}_{\mid i})$ is 
    \[
      Q_{\mid i}(\bm{\theta}_{\mid i}, \bm{\lambda}_{\mid i}) = \sum_{j \ne i}\sum_{k=1}^{n_{u_{Y_i}}}{\left[\log\{\sigma_{j \mid i}(y_{i\mid i, k})\} + \frac{1}{2} \left\{ \frac{y_{j\mid i, k} - \mu_{j\mid i}(y_{j\mid i, k})}{\sigma_{j \mid i}(y_{i\mid i, k})} \right\}^2 \right]}
    \]
  \item Maximise jointly w.r.t $\bm{\theta}_{\mid i}$, $\bm{\lambda}_{\mid i}$, to obtain $\hat{\bm{\theta}}_{\mid i}$ with $\hat{\bm{\lambda}}_{\mid i} $ being nuisance parameters.
  \item For Gumbel margins, must estimate dependence model in two steps, first fixing $\gamma_{j \mid i} = \delta_{j \mid i} = 0$, then estimating both if the indicator function in equation hmm is satisfied.\todo{reference correct equation}
\end{itemize}

\subsubsection{All conditionals}

\begin{itemize}
  \item Falsely assume independence between different conditional distributions: 
    \[
      \hat{Q}( \bm{\theta}, \bm{\lambda}) = \sum_{i=1}^{d}{Q_{\mid i}(\bm{\theta}_{\mid i}, \bm{\lambda}_{\mid i})}
    \]
  \item False assumption above approximates the pseudo-likelihood as marginal density of $\hat{Y}_{-i}$ and conditional density of $\hat{Y}_{-i} \mid Y_i = y_i$ when $y_i < u_{Y_i}$ negligibly influence the shape of the pseudo-likelihood. 
  \item If variables all mutually asymptotically independent, then for sufficiently large $u_{Y_i}$ each datum will exceed at most one threshold, so independence assumption will be satisfied. 
\end{itemize}

\subsection{Uncertainty estimation (bootstrap)}

\begin{itemize}
  \item Uncertainty comes from semiparameteric marginal models, normalising functions and distributions of residuals. 
  \item Semiparametric bootstrap used to evaluate standard errors of model parameter estimates and other estimated parameters such as $\mathbb{P}(\bm{X} \in \bm{C})$. 
  \item Assume marginal and dependence thresholds are fixed, therefore uncertainty due to threshold selection not accounted for. 
  \item Three stages: data generation under fitted model, estimation of model parameters, and derivation of estimate of parameters linked to extrapolation. 
  \item Two-step sampling algorithm used for data generation to replicate both the marginal and dependence features of the data. 
  \item \ldots
\end{itemize}

\todo{Finish this section}

\subsection{Return level}

When multivariate set $\bm{C}$ is described by single parameter $nu$ (i.e.\ $C = C(\nu)$), the return level $\nu_p$ for event with probability $p$ is defined as 
\[
  \mathbb{P}(\bm{Y} \in C(\nu_p)) = p
\]

\subsection{New constraints}

Some new constraints for the conditional extremes model were introduced in \cite{Keef2013}.

\begin{itemize}
  \item The coefficient of tail dependence between the pair of variables $(X_i, X_j)$ is given by
    \[
      \chi_{ij}(p, q) = \mathbb{P}\{X_j > F_j^{-1}(q) \mid X_i > F_i^{-1}(p)\} \text { for } p, q \in (0, 1)
    \]
  \item The limiting positive dependence between two variables is given by 
    \[
      \chi^+_{ij}\lim_{p \rightarrow 1}{\chi_{ij}(p, p)}
    \],
    while the limiting negative dependence is 
    \[
      \chi^-_{ij} = \lim_{p \rightarrow 1}{1-\chi_{ij}(1 - p, p)}
    \]
  \item Asymptotic positive dependence between pair $(X_i, X_j)$ gives $\chi^+_{ij} >0 1$, while asymptotic independence gives $\chi^+_{ij} = 0$.
  \item Similarly, asymptotic negative dependence gives $\chi^-_{ij} > 0$, while asymptotic independence gives $\chi^-_{ij} = 0$.
  \item We must preserve the bounds implied by the asymptotic dependence through a stochastic ordering of the conditional distributions of $Y_j \mid Y_i = y$ for large $y$ associated with asymptotic negative dependence, asymptotic independence, and asymptotic positive dependence, respectively. 
  \item Otherwise, the resulting joint probabilities can exceed the marginal probabilities, i.e.\
    \[
    \hat{\mathbb{P}}(X_i > F_i^{-1}(p),X_j > F_j^{-1}(q)) > \max(1-p, 1-q)
    \]
  \item Let the $q^{th}$ conditional quantile of $Y_j \mid Y_i = y$ for large $y$ be $y_{j \mid i}(q)$ and quantiles under the assumption of asymptotic positive and negative dependence be $y^+_{j \mid i}(q)$ and $y^-_{j \mid i}(q)$, respectively. 
    Then
    \begin{equation} \label{eq:ordering}
      y^-_{j \mid i}(q) \le y_{j \mid i}(q) \le y^+_{j \mid i}(q)
    \end{equation}
    for
    \begin{align*}
      y^-_{j \mid i}(q) &= -y + \bm{Z}_{j \mid i}^-(q), \\
      y_{j \mid i}(q) &= \alpha_{j \mid i}y + y^{\beta_{j \mid i}}(q), \\
      y^+_{j \mid i}(q) &= y + \bm{Z}_{j \mid i}^+(q),
    \end{align*}
    where
    $\hat{G}^-_{j\mid i}\{Z^-_{j \mid i}(q)\} = 
    \hat{G}_{j\mid i}\{Z_{j \mid i}(q)\} = 
  \hat{G}^+_{j\mid i}\{Z^+_{j \mid i}(q)\} = q$, where the $\hat{G}^-_{j \mid i}, \ldots$ are the estimated empirical distributions of $\bm{Z}_i$ for $Y_i > u$ under the assumption of asymptotic negative dependence, etc.
  \item Under the assumption of the asymptotic dependence for $(Y_i, Y_j)$ with $Y_i > u$, $Z^+_{j \mid i}(q)$ is the empirical $q^{th}$ quantile of $Z^+_{j \mid i} = Y_j - Y_i \text{ for } Y_i > u$. \\
  \item Similarly, $\ldots Z^-_{j \mid i}(q) = Y_j + Y_i$. \\
  \item Under the conditional extremes model, we estimate $Z_{j \mid i}(q)$ as the empirical $q^{th}$ quantile of $Z_{j \mid i} = (Y_j - \alpha_{j \mid i}Y_i) / Y_i^{\beta_{j \mid i}}$
  \item The theorem governing this ordering constraint is given by: \\
  \todo{Fix theorem environment}
  % \begin{theorem}
    For $\nu > u$, the ordering constraint \ref{eq:ordering} holds for all $y > \nu$ iff both Case 1 and II hold. \\
    \textbf{Case 1}: Either 
    \[
      \alpha_{j \mid i} \le \min\{
      1, 
      1 - \beta_{j \mid i}Z_{j \mid i}(q)\nu^{\beta_{j \mid i} - 1}, 
      1 - \beta_{j \mid i}Z_{j \mid i}(q) + \nu^{-1}Z^+_{j \mid i}(q)
      \}
    \]
    or 
    % \todo{Reference constraint above which orders ys}
    \todo{Finish theorem! Actually painful to type out lol}
  % \end{theorem}
  \item We only impose constraints on extrapolation to give greatest flexibility $\implies$ take $v$ to be above the maximum observed $Y_i$. 
  \item The constraints are built into the inference by having the profile likelihood for $(\alpha_{j \mid i}, \beta_{j \mid i})$ obtained by MLE equal 0 if Theorem 1 is not satisfied.
  \item Also reduces variance of estimators by removing inconsistent estimates. 
\end{itemize}

\subsubsection{Application}

\subsection{Comparisons to other methods}
\todo{cite Tawn paper comparing CE to max-stable and Gaussian processes}
\todo{Do this section, try summarise, don't have to have too much detail}

\section{Applications of conditional extremes model}

\section{Bayesian spatial clustering for extremes} 
\todo{Make this a subsection of a clustering section!}

This algorithm for Bayesian spatial clustering of (hydrological) extremes is detailed in \cite{Rohrbeck2021}.
\begin{itemize}
  \item Clustering is done for two main reasons:
    \begin{itemize}
      \item interpretation
      \item improve inference by pooling information over similarly distributed variables.
    \end{itemize}
  \item The first is usually done for extremes by fitting multiple marginal GPDs and applying generic clustering techniques (k-means, k-mediods, etc) on some summary statistic (such as the scale parameter, $\sigma$). 
  \item The second often uses hierarchical modelling. Naturally in extreme analyses we have a lack of data, so pooling similar sites etc.\ for parameter estimation is highly advantageous. 
  \item Existing spatial clustering focuses on either the marginal distributions or the dependence structure. 
  \item This Bayesian clustering algorithm combines both ((1) GPD and (2) $\chi$) into likelihood, with reversible jump MCMC algorithm used to estimate cluster allocation and cluster specific marginal parameters. 

\subsection{Model}

\subsubsection{Data}

\begin{itemize}
  \item K sites with spatial locations $s_1, \ldots, s_k \in \mathbb{R}^2$.
  \item For areal data, $s_i$ is the centroid of the $k^{th}$ areal unit. 
  \item For geostatistical data, $s_i$ is the point location of $k^{th}$ site. 
  \item Distance $d_{k, k'} \ge 0$ between sites $k$ and $k'$.
  \item Declustering performed for each site, as data is seasonal and spatio-temporally dependent - only use highest observation per subperiod for which sitewise data is broken into..
  \item $R_{k, 1}, \ldots, R_{k, t}$ denotes the time series for site $k$ after declustering. 
  \item Assumed independent $\forall t \ne t'$. 
  \item $K$ latent variables $\bm{Z} = (Z_1, \ldots, Z_k)$, where $Z_k$ is the latent variable representing the cluster membership for site $k$.
  \item Let $J \in \{1, \ldots, K\}$ represent the number of clusters (so $J$ is also a random variable!).
  \item Cluster based on (1) similar marginal distributions and (2) spatial dependence (represented with $\chi_{k, k'}$) being greater between sites in the same cluster than sites in different clusters. 
\end{itemize}

\subsubsection{Marginal Model}

\begin{itemize}
  \item $R_{k, t} - u_k \mid R_{k, t} > u_k \sim \text{GPD}(\psi_k, \nu_k)$, so we have site-specific marginal parameters and thresholds. 
  \item In clustering, want cluster-specific, rather than site-specific GPD parameters, i.e.\ 
  \[
    R_{k, t} - u_k \mid (Z_k = j, R_{k, t} > u_k) \sim \text{GPD}(\sigma_j, \xi_j).
  \]
  Note here that we still have site-specific thresholds. \todo{Need to unify notation here}
\item Parameters of marginal model given $\bm{Z}$ denoted by $\bm{\theta}_m^{(J)} = \{\bm{\sigma}^{(J)}, \bm{\psi}^{(J)}\}$, where $\bm{\sigma}^{(J)} = (\sigma_1, \ldots, \sigma_J)$ and $\bm{\psi}^{(J)} = (\psi_1, \ldots, \psi_J)$. \todo{have J superscript within bm call}
\end{itemize}

\subsubsection{Dependence Model}

\begin{itemize}
  \item Rather than model full joint distribution over extreme events at sites, model $\chi_{k, k'}$ for pairwise extremal dependence ($\chi_{k, k'}$ defined above) \todo{Have section on multivariate extremes which has a definition for chi}
  \item $\chi_{k, k'} \in [0, 1]$ gives the limit probability of site $k$ observing an extreme event given site $k'$ recording one. 
  \item Constrain $\chi_{k, k'}$, conditional on $\bm{Z}$ such that expected value is larger within clusters than for sites in different clusters: 
  \begin{equation} \label{eq:chi_exp}
    \mathbb{E}(\chi_{k, k'} \mid Z_k = Z_{k'}) \ge \mathbb{E}(\chi_{k, k'} \mid Z_k \ne Z_{k'})
  \end{equation}
  \item Further constrain so that $\chi_{k, k'}$ decreases with increasing distance between sites $d_{k, k'}$, with exponential decay which is also faster for sites in different clusters:
  \begin{equation} \label{eq:chi_dist}
    \mathbb{E}(\chi_{k, k'} \mid Z_k = j, Z_{k'} = j') = \begin{cases}
      \exp(-\gamma_j d_{k, k'}) &\text{if } Z_k = Z_{k'} = j \\
      \exp(-\gamma_0 d_{k, k'}) &\text{if } Z_k \ne Z_{k'}, \\
    \end{cases}
  \end{equation}
  where $\gamma_0 > \max(\gamma_1, \ldots, \gamma_J) \ge 0$, which is ensured by introducing parameters $(\epsilon_1, \ldots, \epsilon_J), \epsilon_j \ge 0$ and constraining the cluster specific decay factors s.t.\ $\log(\gamma_j) = \log(\gamma_0) - \epsilon_j$.
\item $\chi_{k, k'} \mid \bm{Z} \in [0, 1]$ may differ between two pairs of sites and within the same cluster with same $d_{k, k'}$ (due to factors like topology), so we choose a beta distribution model with 
  \todo{Ask Christian about use of Beta distribution here}
  \begin{equation} \label{eq:beta_mod}
    \chi_{k, k'} \mid \bm{Z} \sim \begin{cases}
      \text{Beta}\left(\frac{\beta \exp(-\gamma_j d_{k, k'})}{1 - \exp(-\gamma_j d_{k, k'})}, \beta\right) \text{ if } Z_k = Z_{k'} = j \\
      \text{Beta}\left(\frac{\beta \exp(-\gamma_0 d_{k, k'})}{1 - \exp(-\gamma_0 d_{k, k'})}, \beta\right) \text{ if } Z_k \ne Z_{k'} \\
    \end{cases}
  \end{equation}
  which has expectation equal to \ref{eq:chi_exp}. 
  \item $\beta > 0$ is inversely proportional to the variance of $\chi_{k, k'}$.
  \item Therefore, dependence parameters are $\bm{\theta}_D^{(J)} = \gamma_0, \bm{\epsilon}^{(J)}, \beta\}$. \todo{Why not gamma j? Do we only need to estimate the epsilons that relate them to gamma 0?} 
  \item $\chi_{k, k'}$ only considered for adjacent sites.
  \item For geostatistical data, we derive the Voronoi partition of area and define sites as being adjacent if Voronoi cells are. \mynote{Could be expanded with SPDE approach from INLA? Interesting compromise though}
\end{itemize}
  
\subsection{Inference}

\begin{itemize}
  \item Bayesian inference used to estimate $J, \bm{Z}, \bm{\theta}_m^{(J)}, \bm{\theta}_D^{(J)}$, using declustered data $\bm{D} = \{r_{k, 1}, \ldots, r_{k, t}\}; k = 1, \ldots, K$.
  \item For marginal, data is marginal exceedances over threshold while for dependence the ranks of variables are used. 
  \item Because of this \textcolor{red}{difference in data}, inference for both sets of parameters is largely independent, and we can approximate the joint likelihood using the independent decomposition:
  \[
    L(\bm{\theta}_m^{(J)}, \bm{\theta}_D^{(J)} \mid \bm{D}, \bm{Z}) = L_m(\bm{\theta}_m^{(J)} \mid \bm{D}, \bm{Z}) L_D(\bm{\theta}_D^{(J)} \mid \bm{D}, \bm{Z}).
  \]
\end{itemize}

\subsubsection{Marginal component}

\begin{itemize}
  \item Assuming thresholded data independent over all sites:
    \[
      L_m(\bm{\theta}_m^{(J)} \mid \bm{D}, \bm{Z}) = \prod_{k=1}^{K}{\prod_{\{t:r_{k, t} > u_k\}}{\frac{1}{\sigma_{Z_k}}\left( 1 + \xi_{Z_k} \frac{r_{k, t} - u_k}{\sigma_{Z_k}} \right)_+^{-1/\xi_{Z_k} -1}}}
    \]
    \todo{Ask Christian why the subscript is Zk here, rather than just k?}
  \item However, assuming spatial independence is not a valid assumption, as severe weather events usually affect a number of sites simultaneously. \mynote{There could be an option to include this additional constraint to the marginal component of the likelihood in an R package}
  \item General theory for asymptotic distribution of MLE $\hat{\bm{\theta}}_m^{(J)}$ for GPD parameters under independence assumption is 
    \[
      \sqrt{T}(\hat{\bm{\theta}}_m^{(J)} - \bm{\theta}_m^{(J)}) \sim \text{N}(0, \Sigma = H(\bm{\theta}_m^{(J)})^{-1}V(\bm{\theta}_m^{(J)})H(\bm{\theta}_m^{(J)})^{-1}),
    \]
\end{itemize}
  where $\bm{\theta}_m^{(J)}$ are the true parameter, H denotes the Fischer information (see paper for full definition).
  \item Using $L_m(\bm{\theta}_m^{(J)} \mid \bm{D}, \bm{Z})$ would underestimate the variance of $\bm{\theta}_m^{(J)}$ due to spatial dependence.
  \item Adjust likelihood around it's mode:
    \[
      L_m^{\text{adj}}(\bm{\theta}_m^{(J)} \mid \bm{D}, \bm{Z}) = L_m(\bm{\theta}_m^{(J)} + \bm{B}(\bm{\theta}_m^{(J)} - \hat{\bm{\theta}}_m^{(J)}) \mid \bm{D}, \bm{Z}),
    \]
    where the $2J \times 2J$ matrix $B$ depends on $\bm{Z}$ and is
    \todo{Fix}
    % \[
    %   \bm{B} = \left\{\[H(\bm{\theta}_m^{(J)})\]^{1/2}\right\}^{-1}
    % \]
  \item B is a block matrix of $J 2 \times 2$ blocks for each clustering, allowing for efficient computation. 
  \item Take $ L_m(\bm{\theta}_m^{(J)} \mid \bm{D}, \bm{Z}) =  L_m^{\text{adj}}(\bm{\theta}_m^{(J)} \mid \bm{D}, \bm{Z})$.
  \mynote{i.e.\ use the adjusted likelihood hereafter for the marginal component of the likelihood}
  
  \subsubsection{Dependence component}
  \todo{TODO!}



\printbibliography

\end{document}
