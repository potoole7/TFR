\documentclass{article}

\usepackage[top=3cm, bottom=3cm, left=3cm,right=3cm]{geometry}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{graphicx}
\usepackage{bm}
\usepackage{titlesec}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{natbib} % print author's name and year when citing
\usepackage{bbm}
\usepackage{todonotes}
\usepackage{pdflscape}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{pdfpages}
\usepackage{setspace} 
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage{tikz}
\usepackage[colorlinks=true,citecolor=blue, linkcolor=blue]{hyperref}
\usepackage{multirow}
\usepackage{todonotes}
\setlength{\tabcolsep}{5pt}
%%\setlength{\parindent}{0pt}
\usepackage[parfill]{parskip}
\renewcommand{\arraystretch}{1.5}

% \renewcommand\Affilfont{\itshape\footnotesize}
% \def\ci{\perp\!\!\!\perp}

% \renewcommand\Affilfont{\itshape\footnotesize}
% \linespread{1.5}

% Define a custom note command for general notes
\newcommand{\mynote}[1]{\todo[color=yellow!40,inline]{#1}}

\DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\argmax}{arg\,max}

% Nature Bibliography style
% \usepackage[backend=biber,style=nature]{biblatex}
% \addbibresource{library.bib} 
\bibliographystyle{unsrtnat}

% number equations by section
\numberwithin{equation}{section}

\title{Developing clustering algorithms for conditional extremes models}
\thispagestyle{empty}
\author{Patrick O'Toole \\ SN: 239261652 \\ Supervised by Christian Rohrbeck and Jordan Richards}
% \date{July - September 2024}
\date{\today}

\begin{document}

\begin{center}
  \huge
  \vspace{1.5cm}
  \textbf{Thesis Formulation Report}

  \vspace{0.4cm}
  \huge
  Developing clustering algorithms for conditional extremes models
  
  \Large    
  \vspace{0.8cm}
  \textbf{Patrick O'Toole} \\
  Supervised by: Christian Rohrbeck and Jordan Richards \\
  % July-September 2024
  \today
  \vfill
  \includegraphics[width=7cm]{images/samba.jpg}\\
  \vspace{0.5cm}
  \includegraphics[width=5cm]{images/university-of-bath-logo.png}
  \hspace{1cm}
  \includegraphics[width=5cm]{images/UKRIlogo.png}
  \large   
  \vspace{1.5cm}
\end{center}

\newpage

\todo{Taken from contract, edit to form proper abstract}
\begin{abstract}
  Conditional extreme value models have proven useful for analysing the joint tail behaviour of random vectors. 
  While an extensive amount of work to estimate conditional extremes models exists in multivariate and spatial applications, the prospect of clustering for models of this type has not yet been explored. 
  This project will review existing methods in the area of conditional extremes models, and develop and research ideas on how some of these models can be embedded into a clustering framework. 
  It will also involve the review of existing state-of-the-art clustering methods within extreme value analysis. 
\end{abstract}

% \begin{center}
% \paragraph{Responsible Research and Innovation}\linebreak
% Fill in!
% 
% \end{center}

\todo{Make linebreak the same as abstract}
\todo{Add Responsible Research and Innovation statement}
\todo{Ensure acronyms are defined properly}
\todo{Only have TOC show as far as subsections}

\newpage

\tableofcontents

\newpage

\section{Introduction}\label{sec:intro}

\todo{Look at Coles for more motivation for extremes!}
\todo{Need to define all these equations? Can leave many unumbered}

% \begin{itemize}
%   \item What are extremes (modelling tails of distributions where underlying stochastic process is assumed), why they are used (anywhere where we are more interested in tails of distributions). 
%   \item Examples of uses of extreme value theory (see reading course, Conor Murphy and Matthew Speers review papers).
%   \item Paragraph on key focus of report (dependence modelling through conditional extremes). Could talk about other dependence models, referencing how they are more restrictive and more computationally intensive, citing \cite{Tawn2018} and \cite{Huser2024}. 
%   \item "This proposal suggests combining conditional extremes and clustering", to improve parameter estimation for the model. 
%   \item Summarise what will be talked about in the rest of the report.
% \end{itemize}
\mynote{Key challenges should be left until the end to aid narrative}
\mynote{Keep vague, just mention that clustering for extremes has been done before, but not in the context of conditional extremes}

% Motivating extremes
\todo{Needs some references!}
Often, statistics is concerned with generating models for the mean behaviour of a $\ldots$ \todo{Finish}
% 
For example, being able to predict the expected daily precipitation over the course of a year are essential for both small and large-scale agriculture, and more generally, weather forecasts are key for $\ldots$ the ? of society. \todo{Rewrite, finish}
However, there are situations where we are more interested in only ...
Natural disaster prediction, for example, does not concern itself with the mean behaviour of environmental processes such as precipitation (at least as an outcome), and instead is solely interested in the heaviest precipitation which may cause large scale flooding, and together with very fast wind speeds can lead to storms, and separately for extremely high temperatures can lead to wildfires \todo{Expand on this more}.
This is useful for countries disaster modelling and planning. \todo{Word better, see other papers}.
For example, rivers that are expected to raise to a certain level in the Winter can be lined with sand bags and other flood defences, and the same for areas which are expected to have high winds, such as the coast of Ireland, which can be prepared for with storm barriers and other defences. \todo{Word better}
It is also of use in the insurance industry, where insurance premiums are calculated based on the likelihood of a claim being made, and the size of that claim, and so being able to predict the size of the largest claims, such as the result of flooding, is essential for the industry.
On the other end of the scale, engineers designing and testing a product, such as home utility machines like a washing machine and dishwasher, must keep in mind and identify the components of that product which have the shortest lifespans, as opposed to the mean component lifespan, as this will determine when it will next require fixing, and its ultimate lifespan.
For these applications, a branch of statistics called Extreme Value Theory (EVT) is uniquely suited.
\todo{Talk more about return levels, most common predictions made in EVT}
\todo{Talk about wanting to predict 100 year return levels given only 10 years of data, extrapolating from data we have, you've seen all this before!!}

% What is EVT, examples
EVT focuses on modelling the extreme tails of distributions.
\todo{Add assumption of underlying stochastic process}
It uses asymptotically motivated distributions to model either the maximum observations of a dataset over specified blocks of time, or the observations which exceed a certain threshold. \todo{Read other paper introductions to improve on this!}
More specifically, the Generalised Pareto Distribution (GPD) is used to model exceedances over a certain threshold, and the Generalised Extreme Value (GEV) distribution is used to model the maximum observations over blocks of time.


% Examples
\todo{See Coles intro for examples of uses}

% Dependence 

% Key focus of report, combining conditional extremes and clustering

% Structure of report
Section \ref{sec:motivating} will introduce the motivating example which we will be using throughout this report to illustrate its concepts in an applied setting.
Specifically, we will look at extreme precipitation and wind speed in the Republic of Ireland. It is important to note that this applied context of modelling the bivariate tail behaviour of these two variables across space in relatively novel, as the literature of extremal dependence, conditional extremes and extremal clustering has mainly focused on modelling the spatial dependence of univariate extremes across space. One notable exception of this is in \cite{Vignotto2021}, for which we will take most of our data preprocessing procedure. \todo{Talk about this in so much detail here?}
Following this, section \ref{sec:uni} will start with the concept of marginal extremal modelling and apply it to marginal models for precipitation and wind speed for Ireland.
% Section \ref{sec:uni} will introduce the peaks-over-threshold method for univariate extremes, and show how this can be applied to the motivating example of extreme wind speed and rain in Ireland.
Section \ref{sec:ce} will introduce the conditional extremes model, and show how this can be applied to the motivating example. \todo{Word better}
Section \ref{sec:clustering} will review clustering methods for extremes.
Finally, section \ref{sec:discussion} will summarise the report and suggest future work.
\todo{Flow and wording can be improved a lot!}

\section{Motivating example} \label{sec:motivating}

% \begin{itemize}
  % \item Introduce Ireland dataset, which will serve as a motivating example to elucidate how both marginal extreme value models and the conditional extremes can be fit to data. 
  % \item Precipitation data from Met Eireann, wind speed data from ERA5 reanalysis, reference accordingly (also map from 
  % \item This dataset is interesting in that it has two variables (wind speed, rain), recorded at each site, and this bivariate setting is unusual in the extremal clustering literature, which often focuses on univariate extremes across space (with one notable exception of \cite{Vignotto2021}). 
  % \item Introduction to dataset (rows, locations, years, etc), weekly Winter precipitation sum and daily wind speed maxima for Ireland from 1990 to 2020 inclusive, following \cite{Vignotto2021}. 
    % Include single exploratory plot to showcase data (left plot could have locations of weather sites, right could have rainfall plotted against wind speed for a given site).
    % \todo{Fill in with points to make about data (see past reports)}
    % % \begin{enumerate}
    % % \end{enumerate}
    % \item Will now show how marginal models can be fit to this data.
% \end{itemize}

% Introduce dataset

% Summaries of dataset

% Plot of locations on map, with wind speeds and rainfall for some
Include single exploratory plot to showcase data (left plot could have locations of weather sites, right could have rainfall plotted against wind speed for a given site).

Will now explain univariate extremes show how marginal models can be fit to this data.

\section{Peaks-over-threshold method for univariate extremes}\label{sec:uni}

% Follow other papers using GPD, like \cite{Carreau2017}. 
% \begin{itemize}
%   \item Peaks over threshold (reference Halkema and de Haan 1974, Pickands 1975, see Carreau 2017 for example) definition for extremes (including reference to limiting extreme value distribution $G$ from Coles 2001)
%   \item Conditional CDF of GPD distribution
%   \item Different tails of GPD for different signs of shape parameter
%   \item Talk about different threshold selection methods, e.g.\ stability plots, including quantile regression method used in \cite{Youngman2019}.
%   \item IID assumption in GPD as defined above, can model parameters as covariates, numerous methods to do this, such as using \texttt{evgam} as first introduced in \cite{Youngman2019}, and also Gaussian processes and INLA in \cite{Opitz2018} and Gaussian processes \todo{need reference}.
%   \item Return levels (m-observations, n-year, from \cite{Coles2001}) can be estimated from GPD parameters (derived from conditional probability and exceedance probability, as in \cite{Cooley2007} and \cite{Coles2001}). 
%   \item Reference difficulty in estimating GPD and particularly $\xi$ due to lack of data and strange likelihood (following \cite{Coles2001}). 
%   \item Also mention inadequacy of return level estimates with univariate
%   methods when dependence is seen, as seen in Pacific Northwest in
%   \cite{Zhang2024}, motivating use of dependence models. Also, interest in
%   situations where we have both extreme wind speed and rain, as in
%   \cite{Heffernan2004}, motivating dependence modelling. (Leave for dependence
%   modelling section!)
%   \item (See \cite{Huser2024} for details) Although max-stable naturally follow theory as dependence model, practically they are unwieldy and CE appears to be preferred now in literature. 
% \end{itemize}

% \begin{itemize}
%   \item Suppose we have a continuous random variable X.
%   \item Peaks over threshold definition (see Coles, Carraeu, anything else? Zhang)
% \end{itemize}

\subsection{Theory}

Here we will asymptotically motivate the use of EVT and the GPD \todo{return and write something about this}


\todo{Use Z or X? Z used here to avoid confusion with X later}
% \todo{Split into subsections!?}

\subsubsection{Block maxima asymptotic justification} \label{subsubsec:asymptotic}

\todo{Completely copied from Coles 2001, change to own words, perhaps simplify}
\todo{Dupuis2023 has another good example of this, add reference}

Extreme Value Theory is justified and motivated by the asymptotic basis upon which it relies.
Following the illustration given in \cite{Coles2001}, suppose we have a sequence of independent and identically distributed (IID) random variables $Z_1, Z_2, \ldots$ a distribution function $F$. 
Let $M_n = \max(Z_1, \ldots, Z_n)$ be the maximum of the first $n$ observations. % use Z to not confuse with X later
Then the distribution function of $M_n$ is given by 
\[
  % \mathbb{P}(M_n \le z) = \prod_{i = 1}^{n}{\mathbb{P}(Z_i < z)} = {F(z)}^n.
  \mathbb{P}(M_n \le z) = {F(z)}^n.
\]
Clearly, even slight inaccuracies in the estimate of $F$ can result in significant differences for $F^n$.
Instead, we want to model $F^n$ directly by looking at its limiting behaviour as $n \to \infty$.
Define the upper end-point of F as $z_+ = \sup(F)$.
Then, for any $z < z_+$, $M_n$ will tend to degenerate to point mass on $z_+$ as $n \to \infty$.
This degeneration can be avoided by a linear normalisation of $M_n$:
\begin{equation}
  M_n = \frac{M_n - b_n}{a_n},
\end{equation}
for some sequences of normalising constants $a_n > 0$ and $b_n$.

The Fisher-Tippet-Gnedenko theorem (\cite{Fisher1928}, \cite{Gnedenko1943}) states that the limiting or distribution of these non-degenerate maxima,
\begin{equation} 
  \lim_{n \to \infty}\left\{\frac{M_n - b_n}{a_n}\right\} = G(z),
\end{equation}
\todo{Talk about how Gz is referred to as an extreme value distribution}
is in the max-domain of attraction, i.e.\ belongs to, the Generalised Extreme Value (GEV) distribution. \todo{word better}
We will not define the GEV distribution here for the sake of brevity. 
\todo{Refer to an, bn needing to exist?}
This asymptotic argument is known as the ``block-maxima'' definition of extremes and forms the basis on which the field has developed.

\subsubsection{Peaks-over-threshold method}
\todo{Change title to include GPD?}

% In constrast, GPD for excesses over threshold (Fisher, Tippet, Gnedenko)
From this block-maxima definition, another approach to EVT, which we will be focusing more on, can be derived. 
% Specifically, it rather than taking a block maxima over some sequence, we ca
The Pickands-Balkema-de-Haan theorem (\cite{Pickands1975}, \cite{Balkema1974}) states that should $M_n$ satisfy the above \todo{reference equations} and convergence in distribution to $G(z)$, then for any $Z = Z_i$ in our sequence, the distribution of exceedances $z > 0$ over a high threshold $u$ are approximated by the Generalised Pareto Distribution (GPD), defined as
\begin{equation} \label{eq:gpd}
  % \mathbb{P}(Z > z + u \mid Z > u) = \left(1 + \xi \frac{z}{\sigma} \right)_{+}^{-1/\xi},
  \mathbb{P}(Z > z + u \mid Z > u) = \left(1 + \xi \frac{z}{\sigma} \right)_{+}^{-1/\xi},
\end{equation}
\todo{Check if right! May be definition of Z < z + u, rather than the one I want!}
where $\xi \ne 0$ and $\sigma > 0$ are shape and scale parameters, respectively. \todo{Define y+, ensure it doesn't mess with definition of supremum above}.
The GPD can be derived from the definition of the GEV quite simply, and is covered in detail in section 4.2.2 of \cite{Coles2001}. 
This is known as the ``peaks-over-threshold'' method for modelling extremes. 
In many applications, it is preferable to the block-maxima method, as logically there may be many observations within a single block which exceed so-called ``maxima'' in other blocks \todo{go into more detail here}

\subsubsection{Shape parameter}

% The shape parameter in particular describes the upper tail behaviour of this distribution, and indeed the form of the distribution itself, with the Generalised Pareto distribution actually defining a ``family'' of distributions. 
The upper tail behaviour of this distribution is largely determined by shape parameter. 
Indeed, it determines the form of the distribution itself, with the Generalised Pareto distribution actually defining a ``family'' of distributions. 
Higher values for the shape parameter give higher probabilities of extreme events of greater magnitude. 
When $\xi < 0$, the upper tail is said to follow theWeibull distribution, which is short-tailed and has a finite maximum bounded by the point $u - \sigma/\xi$. 
In the limit $\xi \to 0$, the GPD converges to the exponential distribution, and the distribution is said to be ``light-tailed'' with the right hand side of \ref{eq:gpd} being $ \approx 1/exp(-z)$ for large $z$, with no upper bound on the maximum.
Finally, where $\xi > 0$, the upper tail is said to be follow a Pareto or heavy-tailed distribution, and the distribution function of the GPD is $\approx 1/z^{1/\xi}$ for large $z$, a power-law tail decay. \cite{Rohrbeck2021} \cite{Carreau2017}.

\subsubsection{Threshold selection} \label{subsubsec:threshold}
% Threshold selection methods (stability plots, automatic selection, quantile regression)
% Bias-variance tradeoff in threshold selection
% Can also work with extremal mixture models with so called "Bulk" distributions (as seen in marginal component of CE model)
% (see Rohrbeck2021)
\todo{Rewrite, largely taken from IRP report}
The threshold $u$ can also be thought of as a location parameter for this pdf, as for fixed $\sigma, \xi$, a change in $u$ will simply shift the distribution. \cite{Coles2001}
The problem of threshold selection for the GPD is a difficult one, as the choice of threshold can have a large impact on the parameter estimates of the GPD, and presents a classic bias-variance trade-off. 
A choice of $u$ which is too small can violate the asymptotic properties on which the GPD relies, and introduce significant bias. \todo{Refer to above}
On the other hand, we require sufficient data to estimate the distribution. 
Hence, too large a choice of $u$ will leave us with little data to estimate our distribution with and result in high uncertainty in our parameters. 
In practice, the choice of $u$ comes down to choosing a value large enough to give stable estimates for $\sigma$ and $\xi$, while still leaving enough data to fit the distribution to. 
Two commonly used diagnostic tools to aid in the choice of $u$ are the so-called mean residual life and threshold choice plots \cite{Coles2001}.

The mean residual life plot plots the mean excesses over a series of thresholds.
In mathematical notation, we plot the points
\todo{Check if notation matches that of rest of section}
\todo{Even required if we're not using it?}
\[
  \left\{\left( u, \frac{1}{n_u} \sum_{i=1}^{n_u}{\left(z_i - u\right)}: u < z_{+} \right) \right\} 
\]
\todo{Check with supremum above}
The mean excesses should depend linearly on $u$ for choices of $u$ which produce a stable GPD. \cite{Coles2001} 
The threshold choice plot shows the values of $\xi$ and a modified $\sigma$ for various choices of $u$. 
The choice of $u$ follows as the smallest value which appears to result in stable estimates of these parameters, i.e.\ they should be linear in $u$. 
Both of these methods, however, are rather subjective, and entirely manual, and so are quite laborious in the context of fitting multiple marginal models to, for example, different spatial locations or seasons of the year. 
There are several methods for automatic and ``semi-automatic'' threshold selection. 
One such method is that described in \cite{Murphy2024}, which repeatedly fits the GPD over various choices of $u$ based on the quantiles of the data, and takes the optimal choice to be the threshold which minimises the so-called expected quantile discrepancy (EQD).
This method has been shown to be robust in the face of this bias-variance trade-off problem, when compared to other means of automatic threshold selection, and is less subjective than the plotting methods described above. 

Yet another method for threshold selection is via quantile regression, as outlined in \cite{Youngman2019} in the context of extremes, and more generally developed in \cite{Yu2001}. 
% This is useful where we want to estimate a separate threshold for a number of
% marginal fits, such as to GPDs fitted to different locations. \todo{Can word
% better} (add back in ?)
% Quantile regression estimates $u$ as a function of some covariates, $\bm{x}$, which could include spatial location and/or time, as a prespecified quantile of the data $\tau$. \todo{Need to define x more clearly}
Quantile regression for extremes estimates $u$ as a prespecified quantile $\tau$ of the data. \todo{Rewrite from Youngman}
\todo{May require reference to covariates for this to make sense!}
The random variable $Z$ can be modelled with an asymmetric Laplace distribution,
\begin{equation} \label{eq:asymmetric_laplace}
  Z \mid u, \sigma \sim ALD(u, \psi),
\end{equation}
% \todo{Used sigma already for GPD!}
where $u$ is the quantile $0 < \tau < 1$ of the data, and $\psi > 0$. 
Likelihood methods allow us to estimate $u$ from this model, giving us a threshold above which to define extreme values of $Z$.
This method is very useful when we want to ``semi-automatically'' estimate thresholds across different times and/or spatial locations, which is addressed in greater detail in \ref{subsubsec:non_stationary}.

\todo{expand on this, requires more details}
\todo{Mightn't even make sense without predictors? Wouldn't u just}
% This is the method used in the motivating example for this report.

% Return levels (m-observations, n-year, from Coles)
\subsubsection{Return levels}

\todo{Possibly move this motivation to intro}
One of the principle and indeed unique uses of EVT is in the estimation of return levels, as outlined in \cite{Coles2001}. 
Return levels are immensely useful as they give the expected value of the maximum observation of a random variable $Z$ over a number of observations or block of time, which may be an extrapolation over observed values. \todo{Word better!}
For example, the 100-year return level for precipitation is the value which we expect to be exceeded on average once every 100 years. 
\todo{Talk about how this is useful in applications, or should I do that in intro?}

% m-observation return level
We begin my noting that the probability of $Z$ exceeding some value $z > u$ is given by 
\[
  \mathbb{P}(Z > z) = \mathbb{P}(Z > z \mid Z > u) \mathbb{P}(Z > u)
\]

The $m$-observation return level is the value of $Z$ which is expected to be exceeded on average once every $m$ observations.

The $n$-year return level is the value of $Z$ which is expected to be exceeded on average once every $n$ years.

\todo{Come back to this!}

% Including covariates, using evgam, Gaussian processes, INLA in spatial context
% to reflect non-linearity of spatial effects
\subsubsection{Non-stationary modelling} \label{subsubsec:non_stationary}

\todo{See Rohrbeck2021 for nice write up about this}
Recall that the asymptotic motivations for EVT from \ref{subsubsec:asymptotic} assumes that the distribution of the random variables $Z_1, Z_2, \ldots$ are I.I.D stationary.
In practice, this is often not a realistic assumption, since their values may be influenced by some covariates, such as spatial location or time.
For simplicity, we define these covariates as the spatial location $s \in S$, and $Z(s)$ as the process sampled at $s$, since this is what is used in our motivating example. 

There are many methods for handling this non-stationarity. 
One method is to split the data into separate time series over which we deem the data to be stationary, and fit separate GPDs to each of these series.
However, a more intuitive method which makes better use of the available data is to model the parameters of the GPD.
Firstly, the threshold $u$ can optionally be modelled as a function of covariates $bm{x}$, using the quantile regression method outlined in \ref{subsubsec:threshold}, with equation \ref{eq:asymmetric_laplace} becoming:
\[
  Z(s) \mid u(s), \sigma \sim ALD(u(s), \psi(s)).
\]
This allows us to ``semi-automatically'' select our threshold for different spatial locations.
% \todo{Need to define spatial location as s and use it everywhere! Much simpler}
\todo{Expand on this, mention how its much easier/more objective than stability plots}

Furthermore, we can also model the parameters of the GPD as functions of covariates such as $s$.
Under this model, equation \ref{eq:gpd} becomes
\begin{equation} \label{eq:gpd_cov}
  \mathbb{P}(Z((s)) > z + u(\bm(x)) \mid Z > u, s) = \left(1 + \xi(s) \frac{z}{\sigma(s)} \right)_{+}^{-1/\xi(s)}.
\end{equation}
This allows for much more flexibility in our models, and reduces the bias associated with assuming a constant shape and scale parameter across all locations.
\todo{Also need to revisit return levels, right?}

There are several methods for estimating these models, which in this spatial context are derived from spatio-temporal statistics. \todo{Look in chapter 4 of Coles}
One is to perform regression on the parameters of the GPD, giving the 
% Gaussian processes
% INLA
\todo{Write about INLA and Gaussian processes(?)(?)(?)(?)(?)(?)(?)(?)(?)}

% evgam
\todo{Take from ITT2 report}
Another method which 

\subsubsection{Model diagnostics}
% Difficulty in estimating xi, natural lack of data in extremes
% Instability for certain values (see help page for texmex:evm)
% Uncertainty in parameter estimates from bootstrapping/resampling algorithms in
% Frequentist, or from posterior samples in Bayesian

% Return levels worse for marginal models and want joint extremal probs, need dependence

% Model diagnostics to check how well asymptotic basis holds


\subsection{Application to motivating example}
% \begin{itemize}
%   \item uses \texttt{evgam} to estimate $\sigma, \xi$ at each location smoothing both over space (can take from ITT2 report) for GPDs fitted to precipitation and wind speed, respectively.
%   \item Also uses method from \cite{Youngman2019} whereby location-specific thresholds are defined as fixed quantiles and estimated by quantile regression.
%   \item Show maps of $\sigma$ and $\xi$ for both rain and wind speed, $\ldots$ (what else?)
%   \item Possibly show uncertainty in $\xi$ estimates from vanilla \texttt{texmex}, or can I get uncertainty in \textbf{evgam} estimates for $\xi$ (see \cite{Youngman2023})?
%   \item QQ plots for GPD fits, showing how well the model fits the data (also
%   add histogram and return levels? Will need to fix from texmex plots) \todo{Add uncertainty to plots! From ITT2}
% \end{itemize}

% Method with evgam and quantile regression for threshold selection, to allow
% threshold to vary for different areas. 

% maps of sigma and xi for rain and wind, comment on dynamics

% Uncertainty in xi estimates

% QQ plots for GPD fits 
\todo{Fix texmex so it estimates return levels correctly? Look at where modPoints and sim come from in data, where is ppevm called? Seems to be somethign to with data\$y in evm!}

% Lead into dependence modelling

\section{Conditional extremes model}\label{sec:ce}

% Below narrative is a bit jumbled, can order better by looking at \cite{Heffernan2004}, \cite{Keef2013} and applied conditional extremes papers again. 
% \todo{Anything else to add?}
% \todo{Where to include applications? At the start?}

\subsection{Dependence Modelling}
\begin{itemize}
  \item Define asymptotic dependence/independence, possibly mention $\chi$ statistic. 
  \item Something on $\chi$, follow narrative elsewhere
  \item Modelling dependence give improved return levels and allows for estimation of joint probability of extremes, whether at multiple sites for the same variable or across multiple variables at the same site.
  \item Models include max-stable processes, Pareto processes, Gaussian processes, copulas, and conditional extremes model.
  \item Former four models limited in both their ability to model dependence and their computational feasibility, CE model is more flexible.
  \item Can mention \cite{Tawn2018} and \cite{Huser2024} for more information on this.
\end{itemize}

Section \ref{sec:uni} introduced how marginal models can be fit to data in a univariate extermal context.

\todo{Flesh out, define }
However, may want to estimate joint probability of extremes.
The second part of this equation requires a marginal extremes model, but the first requires a model for the dependence structure. \todo{Rewrite from notes and Heffernan2004}
\begin{equation} \label{eq:joint_prob}
  %\mathbb{P}(\bm{X} \in \bm{C}) = \sum_{i=1}^{d}{\mathbb{P}(\bm{X} \in C_i)} = \sum_{i=1}^{d}\mathbb{P}(\bm{X} \in C_i \mid X_i > u_{X_i}) \mathbb{P}(X_i> u_{X_i})
  \mathbb{P}(\bm{X} \in \bm{C}) = \sum_{i=1}^{d}{\mathbb{P}(\bm{X} \in C_i} = 
  \sum_{i=1}^{d}{\mathbb{P}(\bm{X} \in C_i \mid X_i > u_{X_i}) \mathbb{P}(X_i > u_{X_i}}
\end{equation}
% where $\bm{X}$ is a $d$-dimensional random vector, and $C_i$ is the set of extreme values for the $i^{th}$ component of $\bm{X}$, and $\u_{X_i}$ is the threshold for the $i^{th}$ component of $\bm{X}$.
\todo{Fix above line, how is it producing a missing dollar error??}
This dependence can be for the same variable over space, or different variables at the same location.

Extremal dependence can come in two forms, asymptotic independence and asymptotic dependence
\todo{define, talk about chi statistics}
\todo{Perhaps see Vignotto2021 or Keef2013 for this}
% \begin{itemize}
%   \item 
%     \[
%       \lim_{y \rightarrow \infty}\{\mathbb{P}(\bm{Y}_{-i} \mid Y_i > y)\} = \begin{cases}
%       0 &\text{for asymptotic independence} \\
%       \ne 0 &\text{for asymptotic dependence}, 
%     \end{cases}
%     \]
%   where $\bm{Y}_{-i} = (Y_1, Y_2, \ldots, Y_{i-1}, Y_{i+1}, \ldots, Y_d)$. 
%   \item Existing methods for multivariate extremes (e.g.\ max-stable processes, copulas) can only model $\mathbb{P}(\bm{X} \in C)$ under asymptotic dependence.\todo{revisit this and talk a bit more about it!}
% \end{itemize}

\todo{Also talk about how return levels are better when incorporating dependence information}

\subsection{Conditional extremes model}
% \begin{itemize}
%   \item Introduction to dependence modelling for extremes, following \cite{Heffernan2004} for modelling $\mathbb{P}(\bm{X} \in \bm{C})$, i.e.\ an observation which is extreme in more than one dimension, be that for multiple variables at a single location (e.g.\ rain and wind speed) or across multiple locations and/or times. 
%   Note that CDF must be reversed (see equation 1.2 in \cite{Heffernan2004}).
%   \item Following \cite{Tawn2018} and \cite{Heffernan2004}, introduce other methods for modelling dependence in extremes, such as max-stable, Pareto and Gaussian processes, and copulas, and their limitations, both in the restrictiveness of the type of dependence these methods can model, and their computational feasibility.
%   \item Something on marginal model as piecewise ECDF and GPD, which can be fitted using any method mentioned in \ref{sec:uni}. 
%   \item Starting from asymptotic motivation, define conditional extremes model as in \cite{Heffernan2004} and in applied paper as the non-parametric regression equation $Y_{-i} = a_{\mid i}(Y_{\mid i}) + \b_{\mid i}(Y_{\mid i}) Z_{\mid i}$
%   \item Requires Gumbel transformation of margins to have exponential upper tail, but this gives complex form for $a$ for negative dependence, so instead following \cite{Keef2013} Laplace margins are used which have doubly exponential tails and thus have the conceptually simple $a_{\mid i} = \alpha_{\mid i}Y_{\mid i} , b = Y_{\mid i}^{\beta_{\mid i}}$, which can be interpreted as the slope and spread parameters for our semi-parametric regression line (is there a reference I can use which makes this interpretation?). 
%   \item Extrapolation through MC algorithm as in \cite{Heffernan2004} (alternative in \cite{Keef2013}.
%   \item Talk about diagnostics through independence of residuals and tail exceedances in the limit. 
%   \item Uncertainty through bootstrap scheme,
%   \item Return levels $\ldots$ (improved over univariate, as shown in \cite{Zhang2024})
%   \item Additional constraints from \cite{Keef2013} on possible values of $\alpha$ and $\beta$ to ensure stochastic ordering of $Y_j \mid Y_i = y$ for large $y$ 
% \end{itemize}

The conditional extremes model combines a piecewise semiparametric marginal extremes model with a semiparametric regression model for the dependence structure. \todo{Write better}

\todo{Flesh out more!}

The \texttt{texmex} R package provides a ``vanilla'' implementation of the conditional extremes model. \todo{Move to motivating example}

\subsubsection{Marginal model}

The conditional extremes model requires a marginal model for the complete marginal distribution of a random variable $X_i$.
Therefore, realisations above a high threshold $u_{X_i}$ are modelled using a GPD, while the empirical distribution function is used for observations below this threshold.
For simplicity, we leave out the space covariate $s$ in this section, although in practice we model our GPD as in equation \ref{eq:gpd_cov}.
The marginal distribution of $X_i$ under the conditional extremes model is therefore characterised by the piecewise semiparametric model
\begin{equation} \label{eq:piecewise_marg}
  \hat{F}_{X_i}(x) = \begin{cases}
    1 - \{ 1 - \tilde{F}_{X_i}(u_{X_i})\} \left\{1 + \xi_{i}(x - u_{X_i})/\sigma_i\right\}_{+}^{-1/\xi_{i}} & \text{if } x > u_{X_i} \\
    \tilde{F}_{X_i}(x) & \text{if } x \le u_{X_i},
  \end{cases}
\end{equation}
where $\tilde{F}_{X_i}$ is the empirical distribution function of $X_i$.
It is important to note that this marginal model can be fit using any of the methods outlined in section \ref{sec:uni}, including with the use of covariates to model the parameters of the GPD, as in section \ref{subsubsec:non_stationary}.
\todo{Define gam funciton above, say how we've used it to estimate this model}
From equation \ref{eq:piecewise_marg}, we can estimate the probability of $X_i$ exceeding the threshold $u_{X_i}$, handling the second part of equation \ref{eq:joint_prob}.

\subsubsection{Marginal transformation}

In \cite{Heffernan2004}, the marginal

\begin{align*}
  Y_i &= -\log[-\log\{\hat{F}_{X_i}(X_i)\}], i = 1, \ldots, d \\
      &= t_i(X_i; \phi_i, \tilde{F}_{X_i}(X_i)) \\
      &= t_i(X_i),
\end{align*}
where $\phi_i = (\sigma_i, \xi_i)$ are marginal parameters. \\
This gives $\mathbb{P}(Y_i \le y) = \exp(-\exp(-y)) \implies \mathbb{P}(Y_i > y) \sim \exp(-y) \text{ as } y \rightarrow \infty$, so $Y_i$ has an exponential upper tail. 

\subsubsection{Dependence model}

\subsection{Extensions}
\begin{itemize}
  \item Spatial extension in \cite{Wadsworth2018}, which has $\alpha$ which slowly decays as distance between site and some reference site increases, several choices of $\beta$ to fit different tails, and the representation of the residuals as a Gaussian process with some constraints to ensure it's value is 0 when the site in question is the same as the reference site. 
  \item Modified in \cite{Simpson2023} to be part of latent Gaussian class of models, meaning it can be fitted to 1000s of sites, and can be used in geostatistical context.
\end{itemize}

\subsection{Application to motivating example}

(Not necessarily in correct order)
\begin{itemize}
  \item used \texttt{texmex} to fit conditional extremes model on top of marginal models to estimate $\alpha, \beta$ for rain $\mid$ wind speed and vice versa for each location. 
        Vanilla CE used for simplicity of implementation for this motivating example. 
   \begin{enumerate}
     \item Show diagnostic and quantile plots for CE for some location(s),
     \item Show bootstrapped $\alpha$ and $\beta$ values for rain $\mid$ wind speed and wind speed $\mid$ rain for different thresholds, motivating choice of CE threshold at 70th quantile, and fixing $\beta$ at 0.1 so that all variability is in $\alpha$ (also makes interpretation easier, and need for this further highlights need for better parameter estimation through grouping and/or some hierarchical model). 
     \item Bootstrapped values for $\xi$ (under vanilla \texttt{texmex} marginal estimates, rather than \texttt{evgam}) and $\alpha$ show that uncertainty high in both, even when fixing $\beta$. 
     \item Maps of $\alpha$ values conditioning on rain and windspeed, possibly cross-hatch where bootstrapped $\alpha$ values have 95\% CI which intersects 0. 
     \item Plot of $\alpha$ values versus longitude and latitude (possibly coloured by distance to coast), showing how space is main driver in difference (unsurprising as used as only covariate in marginal \texttt{evgam} model). 
   \end{enumerate}
\end{itemize}


\section{Clustering for extremes}\label{sec:clustering}

Mainly lit review on clustering. 
\begin{itemize}
  \item Clustering generally done for two reasons: improved explainability and improved parameter estimation. 
  \item Wealth of literature on clustering for extremes for both of these types. 
\end{itemize}

\subsection{Explanatory clustering}

\begin{itemize}
  \item First kind mainly focuses on deriving distance matrices for some metric (such as marginal GPD parameters, F-madogram, etc) and applying some classical clustering algorithm such as k-mediods. Can talk about different papers which have done this (multiple references to make here).
  \item In particular \cite{Vignotto2021} paper matches our application here, would be a promising method to try on the conditional extremes model. In particular, is really simple to implement, but perhaps won't help so much with parameter estimation problem. 
\end{itemize}

\subsection{Clustering for improved parameter estimation}

\begin{itemize}
  \item Second kind uses hierarchical modelling and generally produces latent, ``data-driven'' group structure, methods for estimating likelihood can be split between Frequentist methods which use some flavour of EM algorithm, and Bayesian which uses MCMC, in particular \cite{Bottolo2003} and  \cite{Rohrbeck2021}, which use RJMCMC (other methods (possibly from outside extrems) include use of ``stick-breaking prior'' and latent Dirichlet allocation, reference).
  \item This kind particularly useful for extremes because of lack of data causing ``naive'' marginal $\xi$ values to have large variance (which in turn feeds into estimates of $\alpha$ and $\beta$ for CE model). 
  \item Earlier paper in this vein is \cite{Cooley2007}, but this only used domain-knowledge to fit two seperate $\xi$ values for different regions (mountainous and plains).
  \item Talk about Frequentist EM papers for GEV (\cite{Dupuis2023}) and GPD (\cite{Carreau2017}), use variants of EM algorithm. 
  \item Talk about \cite{Rohrbeck2021}, how it changes RJMCMC algorithm from \cite{Bottolo2003} and uses GPD rather than PP (review both papers), some reasons why it may be an improvement on Frequentist methods:
    \begin{enumerate}
      \item Use of priors desirable where data is naturally lacking for extremal context (but may also be shunned for being opinionated?), mention use of Penalised complexity prior for $\xi$ in INLA implementation of GPD. 
      \item Inference for the \cite{Carreau2017} is quite conceptually difficult, making use of U-statistics for probability weighted moment estimators and in the GEV paper required a  consistency analysis of the QML methods used
      \item In contrast, inference for Bayesian problem could be seen as somewhat simpler, can be simply expressed through DAG with hyperpriors to enable partial pooling, can more easily limit e.g.\ $\xi$ to more reasonable values, spatial interpolation is easy (but can be made more complex), number of regions/clusters to use can be estimated with within MCMC scheme rather than e.g. cross-validation as in \cite{Carreau2017}.
      \item Any improvements in computational speed? Can talk about INLA implementation of CE and how it allows for modelling of many regions, opens up use in geostatistical context,
      \item Probabilistically defines uncertainty around parameter estimates, which is nice, compared to having to define complicated bootstrapping schemes or other methods to quantify uncertainty in Frequentist setting. 
      \item Also reference context of conditional extremes model, rather than GPD or GEV explicitely (although GPD must be estimated to then estimate CE parameters), probably more easily formulated in hierarchical Bayesian model (how?)
    \end{enumerate}
  \item However, Frequentist method doesn't require specification of priors, which is a controversial subject and can be quite laborious.  
\end{itemize}

\section{Discussion} \label{sec:discussion}

Summary of report:
\begin{enumerate}
  \item Introduced extremes, why they are important, why dependence modelling is important,
  \item Introduced motivating example of extreme wind speed and rain in Ireland,
  \item Showed how univariate extremes can be modelled using GPD, and how parameter estimates can include covariates, and how GPD can be applied to Ireland dataset. 
  \item Introduced dependence modelling and the conditional extremes model, why it's an improvement over previous models, where it may be susceptible to uncertainty in $\xi$ estimates and some previous applications of the model.
  \item Reviewed clustering methods for extremes, shown examples of both types of clustering, highlighted some papers where there is potential to use these methods on the conditional extremes model.
\end{enumerate}

Future work:
\begin{itemize}
  \item Try general clustering (k-means, k-medoids) on conditional extremes modes, in particular trying to implement \cite{Vignotto2021} paper method on Ireland dataset for conditional extremes model, using estimates \todo{Should I elaborate?}
  \item This model has obvious limitations in that it may not be extended past bivariate case \todo{See discussion in paper to see what they had to say about this}.
  \item Potential to try Bayesian clustering approach similar to \cite{Rohrbeck2021}, will involve deriving likelihood for CE model and priors for parameter values. 
  \item Later, can extend clustering to spatial and spatio-temporal CE from \cite{Tawn2018}.
\end{itemize}

\section{Code availability}

\begin{itemize}
  \item Code for analysis available at \url{https://github.com/potoole7/TFR}. \todo{Make sure this is public while TFR report is being reviewed!}
  \item Fork of \texttt{texmex} package available at \url{http://github.com/potoole7/texmex}, adds functionality to fix $\beta$ values and only estimate $\alpha$. \todo{any other functionality added? Also used marginal fits from evgam rather than naive marginal in texmex}
\end{itemize}

\newpage
\bibliography{library}

\end{document}
